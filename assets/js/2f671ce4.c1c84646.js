"use strict";(globalThis.webpackChunkdocusaurus_book=globalThis.webpackChunkdocusaurus_book||[]).push([[785],{2377(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"learning","title":"Chapter 9: Introduction to Robot Learning","description":"This chapter introduces the exciting field of robot learning, where robots acquire new skills and improve their performance through experience, rather than explicit programming.","source":"@site/docs/09-learning.md","sourceDirName":".","slug":"/learning","permalink":"/learning","draft":false,"unlisted":false,"editUrl":"https://github.com/Ameen-0099/humanoid-robotics-book/tree/main/docs/09-learning.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Intelligence: Conversational AI and Task Planning","permalink":"/gpt"},"next":{"title":"Chapter 10: Ethics and Safety in Humanoid Robotics","permalink":"/ethics"}}');var i=t(4848),r=t(8453);const o={sidebar_position:9},s="Chapter 9: Introduction to Robot Learning",l={},c=[{value:"Basic Principles of Robot Learning",id:"basic-principles-of-robot-learning",level:2},{value:"Learning Paradigms",id:"learning-paradigms",level:2},{value:"Reinforcement Learning (RL)",id:"reinforcement-learning-rl",level:3},{value:"Conceptual RL Agent (Q-learning)",id:"conceptual-rl-agent-q-learning",level:4},{value:"Imitation Learning (IL)",id:"imitation-learning-il",level:3},{value:"Scenarios for Robot Learning",id:"scenarios-for-robot-learning",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-9-introduction-to-robot-learning",children:"Chapter 9: Introduction to Robot Learning"})}),"\n",(0,i.jsx)(n.p,{children:"This chapter introduces the exciting field of robot learning, where robots acquire new skills and improve their performance through experience, rather than explicit programming."}),"\n",(0,i.jsx)(n.h2,{id:"basic-principles-of-robot-learning",children:"Basic Principles of Robot Learning"}),"\n",(0,i.jsx)(n.p,{children:"Robot learning combines principles from robotics, machine learning, and artificial intelligence to enable robots to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adapt"}),": Adjust their behavior to new environments or tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automate"}),": Learn complex behaviors that are difficult to hand-code."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improve"}),": Refine their skills over time through practice and feedback."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-paradigms",children:"Learning Paradigms"}),"\n",(0,i.jsx)(n.p,{children:"Several key paradigms drive robot learning:"}),"\n",(0,i.jsx)(n.h3,{id:"reinforcement-learning-rl",children:"Reinforcement Learning (RL)"}),"\n",(0,i.jsx)(n.p,{children:"In reinforcement learning, a robot (agent) learns by interacting with its environment, receiving rewards for desired behaviors and penalties for undesired ones. The goal is to maximize cumulative reward over time."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    A[Agent] --\x3e|Action| B(Environment);\n    B --\x3e|New State| A;\n    B --\x3e|Reward/Penalty| A;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"conceptual-rl-agent-q-learning",children:"Conceptual RL Agent (Q-learning)"}),"\n",(0,i.jsx)(n.p,{children:"Here's a conceptual Python script demonstrating a simple Q-learning agent for a basic robot task (e.g., navigating a grid)."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass GridWorld:\n    def __init__(self, size=5, start=(0,0), goal=(4,4)):\n        self.size = size\n        self.state = start\n        self.goal = goal\n        self.actions = {\'up\': (-1, 0), \'down\': (1, 0), \'left\': (0, -1), \'right\': (0, 1)}\n\n    def reset(self):\n        self.state = (0,0)\n        return self.state\n\n    def step(self, action_name):\n        action_vec = self.actions[action_name]\n        next_state = (self.state[0] + action_vec[0], self.state[1] + action_vec[1])\n\n        # Keep agent within bounds\n        next_state = (max(0, min(self.size - 1, next_state[0])),\n                      max(0, min(self.size - 1, next_state[1])))\n        \n        self.state = next_state\n        \n        reward = -0.1 # Small penalty for each step\n        done = False\n        if self.state == self.goal:\n            reward = 10.0\n            done = True\n        \n        return self.state, reward, done\n\nclass QLearningAgent:\n    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n        self.env = env\n        self.q_table = {} # (state, action) -> Q-value\n        self.alpha = alpha # Learning rate\n        self.gamma = gamma # Discount factor\n        self.epsilon = epsilon # Exploration-exploitation trade-off\n\n    def get_q_value(self, state, action):\n        return self.q_table.get((state, action), 0.0)\n\n    def choose_action(self, state):\n        if np.random.uniform(0, 1) < self.epsilon:\n            return np.random.choice(list(self.env.actions.keys())) # Explore\n        else:\n            q_values = {action: self.get_q_value(state, action) for action in self.env.actions.keys()}\n            max_q = max(q_values.values())\n            # Choose randomly among actions with max Q-value\n            best_actions = [action for action, q in q_values.items() if q == max_q]\n            return np.random.choice(best_actions) # Exploit\n\n    def learn(self, state, action, reward, next_state):\n        old_q = self.get_q_value(state, action)\n        \n        # Max Q-value for next_state\n        next_q_values = [self.get_q_value(next_state, a) for a in self.env.actions.keys()]\n        max_next_q = max(next_q_values) if next_q_values else 0.0\n\n        new_q = old_q + self.alpha * (reward + self.gamma * max_next_q - old_q)\n        self.q_table[(state, action)] = new_q\n\nif __name__ == "__main__":\n    env = GridWorld()\n    agent = QLearningAgent(env)\n\n    num_episodes = 100\n    print(f"Training Q-Learning agent for {num_episodes} episodes...")\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        total_reward = 0\n        while not done:\n            action = agent.choose_action(state)\n            next_state, reward, done = env.step(action)\n            agent.learn(state, action, reward, next_state)\n            state = next_state\n            total_reward += reward\n        \n        if (episode + 1) % 20 == 0:\n            print(f"Episode {episode + 1}: Total Reward = {total_reward:.2f}")\n\n    print("\\nTraining complete. Learned Q-table (sample):")\n    for (s, a), q in list(agent.q_table.items())[:10]: # Print first 10 entries\n        print(f"  Q({s}, {a}) = {q:.2f}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"imitation-learning-il",children:"Imitation Learning (IL)"}),"\n",(0,i.jsx)(n.p,{children:"Imitation learning (also known as Learning from Demonstration) involves a robot learning a skill by observing a human or expert performing the task. The robot tries to mimic the expert's behavior."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\n    E[Expert Demonstrations] --\x3e F(Feature Extraction);\n    F --\x3e M(Learning Model);\n    M --\x3e R[Robot Policy];\n"})}),"\n",(0,i.jsx)(n.h2,{id:"scenarios-for-robot-learning",children:"Scenarios for Robot Learning"}),"\n",(0,i.jsx)(n.p,{children:"Robot learning is particularly effective in scenarios where:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The task is complex and difficult to program manually (e.g., opening a door with an unknown handle)."}),"\n",(0,i.jsx)(n.li,{children:"The environment is unstructured and dynamic (e.g., navigating a cluttered room)."}),"\n",(0,i.jsx)(n.li,{children:"The robot needs to adapt to user preferences or changing requirements."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsx)(n.p,{children:"This chapter laid the foundation for how robots can learn from experience. In the next chapter, we will address the crucial considerations of Ethics and Safety in Humanoid Robotics."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>s});var a=t(6540);const i={},r=a.createContext(i);function o(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);