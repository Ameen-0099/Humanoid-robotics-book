"use strict";(globalThis.webpackChunkdocusaurus_book=globalThis.webpackChunkdocusaurus_book||[]).push([[71],{8453(n,e,a){a.d(e,{R:()=>s,x:()=>r});var o=a(6540);const i={},t=o.createContext(i);function s(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(t.Provider,{value:e},n.children)}},9704(n,e,a){a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"gpt","title":"Chapter 8: Intelligence: Conversational AI and Task Planning","description":"This chapter explores how humanoid robots can move beyond simple pre-programmed actions to engage in natural language conversations and perform complex, goal-oriented task planning.","source":"@site/docs/08-gpt.md","sourceDirName":".","slug":"/gpt","permalink":"/Humanoid-robotics-book/gpt","draft":false,"unlisted":false,"editUrl":"https://github.com/Ameen-0099/Humanoid-robotics-book/tree/master/docs/08-gpt.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Manipulation: Grasping and Interaction","permalink":"/Humanoid-robotics-book/manipulation"},"next":{"title":"Chapter 9: Introduction to Robot Learning","permalink":"/Humanoid-robotics-book/learning"}}');var i=a(4848),t=a(8453);const s={sidebar_position:8},r="Chapter 8: Intelligence: Conversational AI and Task Planning",l={},c=[{value:"Conversational AI in Robotics",id:"conversational-ai-in-robotics",level:2},{value:"Conversational AI Pipeline for a Robot",id:"conversational-ai-pipeline-for-a-robot",level:3},{value:"Task Planning and Execution",id:"task-planning-and-execution",level:2},{value:"Hierarchical Task Planning",id:"hierarchical-task-planning",level:3},{value:"Challenges of Integrating Conversational AI with Physical Actions",id:"challenges-of-integrating-conversational-ai-with-physical-actions",level:3},{value:"Code Example: Simple Conversational Agent (Conceptual)",id:"code-example-simple-conversational-agent-conceptual",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-8-intelligence-conversational-ai-and-task-planning",children:"Chapter 8: Intelligence: Conversational AI and Task Planning"})}),"\n",(0,i.jsx)(e.p,{children:"This chapter explores how humanoid robots can move beyond simple pre-programmed actions to engage in natural language conversations and perform complex, goal-oriented task planning."}),"\n",(0,i.jsx)(e.h2,{id:"conversational-ai-in-robotics",children:"Conversational AI in Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Conversational AI allows robots to understand human language, process requests, and respond in a natural and intelligent manner. This is crucial for intuitive human-robot interaction."}),"\n",(0,i.jsx)(e.h3,{id:"conversational-ai-pipeline-for-a-robot",children:"Conversational AI Pipeline for a Robot"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:"graph TD;\n    H[Human Speech] --\x3e S(Speech Recognition);\n    S --\x3e NLU(Natural Language Understanding);\n    NLU --\x3e DM(Dialogue Manager);\n    DM --\x3e TG(Task Generation);\n    TG --\x3e RP(Robot Planning & Execution);\n    RP --\x3e TTS(Text-to-Speech);\n    TTS --\x3e R[Robot Speech];\n"})}),"\n",(0,i.jsx)(e.h2,{id:"task-planning-and-execution",children:"Task Planning and Execution"}),"\n",(0,i.jsx)(e.p,{children:"Task planning enables robots to break down high-level goals into a sequence of executable actions. For humanoid robots, this often involves considering their physical capabilities and the environment."}),"\n",(0,i.jsx)(e.h3,{id:"hierarchical-task-planning",children:"Hierarchical Task Planning"}),"\n",(0,i.jsx)(e.p,{children:"A common approach is hierarchical task planning, where high-level tasks are decomposed into smaller, more manageable sub-tasks."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:"graph TD;\n    G[High-Level Goal] --\x3e T1(Sub-Task A);\n    G --\x3e T2(Sub-Task B);\n    T1 --\x3e A1(Action 1.1);\n    T1 --\x3e A2(Action 1.2);\n    T2 --\x3e A3(Action 2.1);\n    T2 --\x3e A4(Action 2.2);\n"})}),"\n",(0,i.jsx)(e.h3,{id:"challenges-of-integrating-conversational-ai-with-physical-actions",children:"Challenges of Integrating Conversational AI with Physical Actions"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Grounding"}),": Connecting abstract linguistic concepts to concrete physical actions and perceptions."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Management"}),": Maintaining a coherent understanding of the conversation and the robot's environment."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Ambiguity Resolution"}),": Dealing with vague or incomplete human commands."]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real-time Performance"}),": Ensuring timely responses and actions to maintain natural interaction flow."]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"code-example-simple-conversational-agent-conceptual",children:"Code Example: Simple Conversational Agent (Conceptual)"}),"\n",(0,i.jsx)(e.p,{children:"Here's a conceptual Python script demonstrating a simple conversational agent's interaction. This simplified example focuses on intent recognition and response generation."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ConversationalAgent:\n    def __init__(self):\n        self.knowledge_base = {\n            "hello": "Hello! How can I help you?",\n            "move": "Which direction and how far?",\n            "pick up": "What would you like me to pick up?",\n            "status": "I am currently awaiting your command.",\n            "bye": "Goodbye! Have a great day."\n        }\n\n    def process_command(self, command):\n        command = command.lower()\n        if "hello" in command:\n            return self.knowledge_base["hello"]\n        elif "move" in command:\n            return self.knowledge_base["move"]\n        elif "pick up" in command:\n            return self.knowledge_base["pick up"]\n        elif "status" in command:\n            return self.knowledge_base["status"]\n        elif "bye" in command:\n            return self.knowledge_base["bye"]\n        else:\n            return "I\'m sorry, I don\'t understand that command."\n\nif __name__ == "__main__":\n    agent = ConversationalAgent()\n    print("Robot: " + agent.process_command("Hello there!"))\n    print("Robot: " + agent.process_command("Can you move forward?"))\n    print("Robot: " + agent.process_command("What is your current status?"))\n    print("Robot: " + agent.process_command("Please sing a song."))\n    print("Robot: " + agent.process_command("Bye bye!"))\n'})}),"\n",(0,i.jsx)(e.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsx)(e.p,{children:"This chapter provided insights into how robots can understand and plan. Building on this intelligence, the next chapter will introduce the critical field of Robot Learning, where robots improve their performance through experience."})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);